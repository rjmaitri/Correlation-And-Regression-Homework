---
output: 
  html_document: 
    df_print: kable
    theme: cerulean
---
  
<div align="center">
 <marquee behavior="alternate" bgcolor="#bb3434" direction="left" height:="" 
 loop="7" scrollamount="1" scrolldelay="2" width="100%">
 <span style="font-size: 20px;color:#FFFFFF">
 Correlation and Regression!</span></marquee>
</div>

---
title: "Homework 5"
author: "Bob Bartolini"
date: "10//2020"
output: html_document
  

---

https://github.com/rjmaitri/Homework-2.git

```{r setup, include=FALSE}
#scrolling code output
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```

```{r}
library(readr)
library(reactable)
library(ggplot2)


```

Note: Datasets are available at http://whitlockschluter.zoology.ubc.ca/data so you don’t have to type anything in (and have to load it!)

### 1. Correlation - W&S Chapter 16
Data at https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter16/chap16q15LanguageGreyMatter.csv 

## 1. Does learning a second language change brain structure? Mechelli tested 22 native Italian speakers who had learned English as a second language. Procifiencies in reading, writing, and speech were assesed using a number of tests whose results were summarized by a proficiency score. Gray-matter density was measured in the left inferior parietal region of the brain using a neuroimaging technique, as mm3 of gray matter per voxel. (A voxel is a picture element, or "pixel", in three dimensions.) The data are listed in the accompanying table. 


```{r}
#load the data
grey_proficiency <- read.csv("./data/chap16q15LanguageGreyMatter.csv")
#check the structure of the data
str(grey_proficiency)

```

```{r}


skimr::skim(grey_proficiency)


```

## 1a. Display the association between the two variables in a scatter plot

```{r}

#plot grey matter by proficiency
ggplot(data = grey_proficiency,
       mapping = aes(x = proficiency, y = greymatter)) +
  geom_point()+
  stat_smooth(method =lm)+
  theme_bw()

```

<span style="color: green;"> The greymatter density increases with second language proficiency. </span>

##1b. Calculate the correlation between second language procifiency and grey-matter density

```{r}
#output the correlation matrix of this data
cor(grey_proficiency)

```

<span style="color: green;"> This correlation plot depicts the degree of association between grey-matter density and second language proficiency. We see that the covariance is 0.81.</span>

## 1c. Test the Null Hypothesis of zero correlation.

```{r}
#fit the density-grey matter model
density_lm <- lm(greymatter ~ proficiency, data = grey_proficiency)

summary(density_lm)
```

<span style="color: green;">We fail to accept the null hypothesis of zero correlation, as the p-value is 3.264e-06, which is less than our alpha of 0.05</span>

### 2. Correlation - W&S Chapter 16
Data at https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter16/chap16q19LiverPreparation.csv

## The following data are from a laboratory experiment by Smallwood et al. (1998) in which liver preparations from five rats were used to measure the relationship between the administered concentration of taurocholate (a salt normally occuring in liver bile) and the unbound fraction of taurocholate in the liver.

## 2a. Calculate the correlation between the taurocholate unbound fraction and the concentration.

```{r}
#load the data
liver_data <- read.csv("data/chap16q19LiverPreparation.csv")
#check the structure
str(liver_data)



```

<span style="color: green;">This dataset displays the relationship between administered taurochlorate concentration and the fraction of unbound tuarocholorate in the liver.</span>




```{r}

skimr::skim(liver_data)

```

```{r}
#correlation matrix for liver_data
cor(liver_data)


```

<span style="color: green;">The covariance of -0.85 indicates a strong negative correlation between the increasing concentration alongside a decreasing unbound fraction of taurocholate. The negative value indicates the direction of the effect concentration has on the unbound fraction.</span>

## 2b. Plot the relationship between the two variables in a graph.

```{r}

ggplot(data = liver_data,
       mapping = aes(x = concentration, y = unboundFraction)) +
  geom_point()+
  stat_smooth(method=lm)+
  theme_bw()


```


#### Log-transformation of the unbound fraction might be useful for linearity.

```{r}


ggplot(data = liver_data,
       mapping = aes(x = concentration, y = log(unboundFraction))) +
  geom_point()+
  stat_smooth(method =lm)+
  theme_bw()




```

<span style="color: green;">Residuals appear to have shrunk"</span>


## 2c. Examine the plot in part (b). The relationship appears to be maximally strong, yet the correlation coefficient you calculated in part (a) is not near the maximum possible value. 
Why not?

<span style="color: green;">The negative correlation coefficient approaches the minimum, since it has an increasing and decreasing variables. The correlation coefficient would approach the maximum if both variables were positive. </span>

## 2d. What steps would you take with these data to meet the assumptions of correlation analysis?

```{r}

####arcsine for fraction

#fit the unboundfraction_taurochlorate model
liver_lm <- lm((unboundFraction) ~ concentration, data = liver_data)

summary(liver_lm)

```

```{r}


coef(liver_lm)

```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

#use the fit model to test our data

#does the distribution of our predictions match our data?

unbound_sims <- simulate(liver_lm, nsim = 20) %>%
  pivot_longer(
    cols = everything(),
    names_to = "sim",
    values_to = "unboundFraction"
  )

#look if our predictions match the data
ggplot() +
  #layer the sims
  geom_density(data = unbound_sims, 
               mapping = aes(x = unboundFraction, group = sim), size =  0.2) +
  #layer the data 
   geom_density(data = liver_data, 
               mapping = aes(x = unboundFraction), size =  0.2, color = "blue")

```

<span style="color: green;">Our simulations has some predictions which do not match our data. The small sample size appears to be an issue. Also, the quantities appear to be in different scales.</span>



#is there a relationship between fitted and residual values

```{r}
#plot fitted vs. residual of the unbound fraction model
plot(liver_lm, which =1)

```

<span style="color: green;">The taurocholate data does not have a pattern for the residuals vs. fitted values, however, the small sample size does not give us any information for the scatter of the residual values. This plot would be more useful with replicate experiments.</span>



#### Did we satisfy normality and homoskedascity using a ggplot and levene test

```{r}

#Generate a QQplot
plot(liver_lm, which =2)

```

<span style="color: green;">The error generating process contains a normal distribution, as the residuals trend along a straight line</span>

#### Look for outliers with leverage. 

```{r}

#plot cooks distance to scan for outliers

plot(liver_lm, which =4)


```

<span style="color: green;">Rat #5 has contains an outlier, as it contains a Cook's distance over 1. </span>

```{r}
plot(liver_lm, which = 5)

```

<span style="color: green;">Rat samples 1 and 5 have a considerable amount of leverage</span>

<span style="color: green;">These tests of assumptions contain some minor failures, however replicate experiments should be conducted before reconstructing the model. If further data fails these tests, the taurocholate administration v. unbound fraction model may need to be revised based on these diagnostic tests of assumptions. More predictors, such as proteins relevant to unbound taurocholate may need to be investigated and incorporated to the model. </span>


#### 3. Correlation SE
#### Consider the following dataset:



cats	happiness_score
-0.30	-0.57
0.42	-0.10
0.85	-0.04
-0.45	-0.29
0.22	0.42
-0.12	-0.92
1.46	0.99
-0.79	-0.62
0.40	1.14
-0.07	0.33

```{r}

cats <- c(-0.30,0.42,0.85,-0.45,0.22,-0.12,1.46,-0.79,0.40,-0.07)

happiness_score <- c(-0.57,-0.10,-0.04,-0.29,0.42,-0.92,0.99,-0.62,1.14,0.33)

cats_happiness <- data.frame(cats, happiness_score)

```


#### 3a. Are these two variables correlated? What is the output of cor() here. What does a test show you?


```{r}
#ouput correlation matrix, created object for part 3c.
cat_cor <- cor(cats_happiness)

```


#### 3b. What is the SE of the correlation based on the info from cor.test()

```{r}

cor_test <- cor.test(cats_happiness$cats, cats_happiness$happiness_score)

```

<span style="color: green;">t = r/SE</span>

<span style="color: green;">r = 0.6758, t = 2.5938</span>

<span style="color: green;">2.5938 = 0.6758/SE</span>

<span style="color: green;">2.5938SE=0.6758</span>

<span style="color: green;">SE=0.6758/2.5938</span>

<span style="color: green;">SE=0.26</span>

<span style="color: green;">The standard error is *0.26*</span>


#### 3c. Now, what is the SE via simulation? To do this, you’ll need to use cor() and get the relevant parameter from the output (remember - you get a matrix back, so, what’s the right index!), replicate(), and sample() or dplyr::sample_n() with replace=TRUE to get, let’s say, 1000 correlations. How does this compare to your value above?

```{r}


cats <- replicate(1000, cor(sample_n(cats_happiness, size = nrow(cats_happiness), replace = TRUE))[1,2])


```

<span style="color: green;">The SE of 1000 simulated correlations is `r sd(cats)`</span>



#### 4. W&S Chapter 17
#### Data at https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter17/chap17q19GrasslandNutrientsPlantSpecies.csv
#### You might think that increasing the resources avaiable would elevate the number of plant species that an area could support, but the evidence suggests otherwise. The data in accompanying table are from Park Grass Experiment at Rothamsted Experimental Station in the U.K., where grassland field plots have been fertilized annually for the  past 150 years. The number of plant species recirded in 10 plots is given in response to the number of different nutrient types added in the fertilizer treatment (nutrient types include nitrogen, phosphorus, potassium and so on).

```{r}
plot <-  c(1,2,3,4,5,6,7,8,9,10)
Nutrients <-  c(0,0,0,1,2,3,1,3,4,4)
Species <- c(36,36,32,34,33,30,20,23,21,16)


plant_nutrient <- data.frame(plot, Nutrients, Species)



```

a) Draw the scatter plot of these data. Which variable should be the explanatory variable (X), and which should be the response variable (Y)?

<span style="color: green;">The number of plant species is the response variable (Y) and the explanatory variable is the number nutrients added (X)

```{r}
ggplot(data = plant_nutrient,
       mapping = aes(x = Nutrients, y = Species)) +
  geom_point(colour = "yellow", size =2)+
  theme_dark()

```

#### 4b. What was the rate of change in the number of plant species supported per nutrient type added? Provide a standard error in your estimate.

```{r}
#fit a linear model
Species_nutrients_mod <- lm(Species ~ Nutrients, data =plant_nutrient)
#find the slope of the line
print(Species_nutrients_mod)
```

<span style="color: green;">Species decrease by 3.339 for each nutrient added to a plot.</span>

```{r}

cor_plants <- cor.test(plant_nutrient$Nutrients, plant_nutrient$Species)


cor_plants

```


<span style="color: green;">Calculate standard error of the slope using the formula SE=slope/t gives `r -3.339/-3.0398` </span>

#### 4c. Add the least-squares regression to your scatter plot. What fraction of the variation in the number of plant species is "explained" by the number of nutrients added?

```{r}

ggplot(data = plant_nutrient,
       mapping = aes(x = Nutrients, y = Species)) +
  geom_point(colour = "yellow", size =2)+
  stat_smooth(method =lm, fill="pink", colour="green", size=1)+
  theme_dark()

```

<span style="color: green;">The coefficient of determination, r<sup>2</sup> = `r summary(Species_nutrients_mod)$r.squared`</span>

#### 4d. Test the null hypothesis, that no treatment effect on the number of plant species.

<span style="color: green;">We fail to accept the null hypothesis (p:value = `r cor_plants$p.value`) that there is no treatment effect on the number of plant species.</span>

5. W&S Chapter 17-25
https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter17/chap17q25BeetleWingsAndHorns.csv 



Do any other diagnostics misbehave?
6. W&S Chapter 17-30
https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter17/chap17q30NuclearTeeth.csv 

Using predict() and geom_ribbon() in ggplot2, reproduce the above plot showing data, fit, fit interval, and prediction interval.
EC. Intervals and simulation
Fit the deet and bites model from lab.



Now, look at vcov() applied to your fit. For example:

deet_mod <- lm(bites ~ dose, data = deet)

vcov(deet_mod)
##             (Intercept)         dose
## (Intercept)  0.09929780 -0.025986850
## dose        -0.02598685  0.007437073
What you have here is the variance-covariance matrix of the parameters of the model. In essence, every time you larger slopes in this case will have smaller intercepts, and vice-verse. This maintains the best fit possible, despite deviations in the slope and intercept. BUT - what’s cool about this is that it also allows us to produce simulations (posterior simulations for anyone interested) of the fit. We can use a package like mnormt that let’s us draw from a multivariate normal distribution when provided with a vcov matrix. For example…

library(mnormt)

rmnorm(4, mean = coef(deet_mod), varcov = vcov(deet_mod))
##      (Intercept)       dose
## [1,]    3.541639 -0.3524774
## [2,]    4.061716 -0.5296708
## [3,]    4.010105 -0.4772817
## [4,]    4.112800 -0.4790570
produces a number of draws of the variance and the covariance!

ECa. Fit simulations!
Using geom_abline() make a plot that has the following layers and shows that these simulated lines match up well with the fit CI. 1) the data, 2) the lm fit with a CI, and 3) simulated lines. You might have to much around to make it look as good as possible.

ECb. Prediction simulations!
That’s all well and good, but what about the prediction intervals? To each line, we can add some error drawn from the residual standard deviation. That residual can either be extracted from summary() or you can get the sd of residuals.

Now, visualize the simulated prediction interval around the fit versus the calculated prediction interval around the fit via predict. +1 extra credit for a clever visualization of all elements on one figure - however you would like